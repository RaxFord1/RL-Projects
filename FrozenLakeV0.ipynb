{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FrozenLakeV0.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaxFord1/RL-Projects/blob/master/FrozenLakeV0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jeWo3dVKp4R",
        "colab_type": "text"
      },
      "source": [
        "To save video, we need X screen, suddenly colab is a cloud-service and we don't have screen at all, so we need to simulate it with xvfb lib"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fw_IMFC_Q2KM",
        "colab": {}
      },
      "source": [
        "!apt-get install -y xvfb python-opengl > /dev/null 2>&1\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython import display as ipythondisplay"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12rI-xMcLBKs",
        "colab_type": "text"
      },
      "source": [
        "Creating virtual screen"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3df52e9b-5a04-4a90-a030-f7441b0781c9",
        "id": "GtoFd5S1Q2KO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(400, 300))\n",
        "display.start()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "xdpyinfo was not found, X start can not be checked! Please install xdpyinfo!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1013'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1013'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUTrkqSMLIzB",
        "colab_type": "text"
      },
      "source": [
        "**Importing all needs `gym, pytorch, numpy`**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C2nlpi8eQ2KS",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import gym\n",
        "import gym.spaces\n",
        "import gym.wrappers\n",
        "import gym.envs.toy_text.frozen_lake\n",
        "from collections import namedtuple\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC8jNhAoePNC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DiscreteOneHotWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super(DiscreteOneHotWrapper, self).__init__(env)\n",
        "        assert isinstance(env.observation_space, gym.spaces.Discrete)\n",
        "        self.observation_space = gym.spaces.Box(0.0, 1.0, (env.observation_space.n, ), dtype=np.float32)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        res = np.copy(self.observation_space.low)\n",
        "        res[observation] = 1.0\n",
        "        return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkZgBov9Qvm2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = gym.envs.toy_text.frozen_lake.FrozenLakeEnv(is_slippery=False)\n",
        "env = gym.wrappers.TimeLimit(env, max_episode_steps=100)\n",
        "env = DiscreteOneHotWrapper(env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nggRq4K6VVrd",
        "colab_type": "code",
        "outputId": "cce442b5-798d-4504-dec1-17159cef9754",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "env.observation_space.shape[0]"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FemAbVGVkJu",
        "colab_type": "code",
        "outputId": "c3f2a806-caa8-4c08-fa7b-4b5608cf9a8c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "env.action_space.n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M60VT2FwRAY5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HIDDEN_SIZE = 128\n",
        "BATCH_SIZE = 100\n",
        "PERCENTILE = 30\n",
        "GAMMA = 0.99"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnKR5mx0RA_r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self, obs_size, hidden_size, n_actions):\n",
        "        super(Net, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(obs_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, n_actions)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95aj68ceRAw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Episode = namedtuple('Episode', field_names=['reward', 'steps'])\n",
        "EpisodeStep = namedtuple('EpisodeStep', field_names=['observation', 'action'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBdvR98vSUU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_batches(env, net, batch_size):\n",
        "    batch = []\n",
        "    episode_reward = 0.0\n",
        "    episode_steps = []\n",
        "    obs = env.reset()\n",
        "    sm = nn.Softmax(dim=1)\n",
        "    while True:\n",
        "        obs_v = torch.FloatTensor([obs])\n",
        "        act_probs_v = sm(net(obs_v))\n",
        "        act_probs = act_probs_v.data.numpy()[0]\n",
        "        action = np.random.choice(len(act_probs), p=act_probs)\n",
        "        next_obs, reward, is_done, _ = env.step(action)\n",
        "        episode_reward += reward\n",
        "        episode_steps.append(EpisodeStep(observation=obs, action=action))\n",
        "        if is_done:\n",
        "            batch.append(Episode(reward=episode_reward, steps=episode_steps))\n",
        "            episode_reward = 0.0\n",
        "            episode_steps = []\n",
        "            next_obs = env.reset()\n",
        "            if len(batch) == batch_size:\n",
        "                yield batch\n",
        "                batch = []\n",
        "        obs = next_obs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrcBSHAxdmKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_batch(batch, percentile):\n",
        "    disc_rewards = list(map(lambda s: s.reward * (GAMMA ** len(s.steps)), batch))\n",
        "    reward_bound = np.percentile(disc_rewards, percentile)\n",
        "\n",
        "    train_obs = []\n",
        "    train_act = []\n",
        "    elite_batch = []\n",
        "    for example, discounted_reward in zip(batch, disc_rewards):\n",
        "        if discounted_reward > reward_bound:\n",
        "            train_obs.extend(map(lambda step: step.observation, example.steps))\n",
        "            train_act.extend(map(lambda step: step.action, example.steps))\n",
        "            elite_batch.append(example)\n",
        "\n",
        "    return elite_batch, train_obs, train_act, reward_bound"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4sfjlM2SdH1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7fb752da-236e-4b7f-abd0-21fdc78b6a3a"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    random.seed(12345)\n",
        "\n",
        "    obs_size = env.observation_space.shape[0]\n",
        "    n_actions = env.action_space.n\n",
        "\n",
        "    net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
        "    objective = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(params=net.parameters(), lr=0.001)\n",
        "\n",
        "    full_batch = []\n",
        "    for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
        "        reward_mean = float(np.mean(list(map(lambda s: s.reward, batch))))\n",
        "        full_batch, obs, acts, reward_bound = filter_batch(full_batch + batch, PERCENTILE)\n",
        "        if not full_batch:\n",
        "            continue\n",
        "        obs_v = torch.FloatTensor(obs)\n",
        "        acts_v = torch.LongTensor(acts)\n",
        "        full_batch = full_batch[-500:]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        action_scores_v = net(obs_v)\n",
        "        loss_v = objective(action_scores_v, acts_v)\n",
        "        loss_v.backward()\n",
        "        optimizer.step()\n",
        "        print(\"%d: loss=%.3f, reward_mean=%.3f, reward_bound=%.3f, batch=%d\" % (\n",
        "            iter_no, loss_v.item(), reward_mean, reward_bound, len(full_batch)))\n",
        "        if reward_mean > 0.99:\n",
        "            print(\"Solved!\")\n",
        "            break\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1: loss=1.377, reward_mean=0.020, reward_bound=0.000, batch=2\n",
            "2: loss=1.370, reward_mean=0.000, reward_bound=0.000, batch=2\n",
            "3: loss=1.368, reward_mean=0.040, reward_bound=0.000, batch=6\n",
            "4: loss=1.357, reward_mean=0.020, reward_bound=0.000, batch=8\n",
            "5: loss=1.353, reward_mean=0.030, reward_bound=0.000, batch=11\n",
            "6: loss=1.347, reward_mean=0.020, reward_bound=0.000, batch=13\n",
            "7: loss=1.350, reward_mean=0.050, reward_bound=0.000, batch=18\n",
            "8: loss=1.344, reward_mean=0.040, reward_bound=0.000, batch=22\n",
            "9: loss=1.339, reward_mean=0.050, reward_bound=0.000, batch=27\n",
            "10: loss=1.334, reward_mean=0.040, reward_bound=0.000, batch=31\n",
            "11: loss=1.328, reward_mean=0.040, reward_bound=0.000, batch=35\n",
            "12: loss=1.322, reward_mean=0.010, reward_bound=0.000, batch=36\n",
            "13: loss=1.323, reward_mean=0.040, reward_bound=0.000, batch=40\n",
            "14: loss=1.317, reward_mean=0.030, reward_bound=0.000, batch=43\n",
            "15: loss=1.313, reward_mean=0.030, reward_bound=0.000, batch=46\n",
            "16: loss=1.307, reward_mean=0.050, reward_bound=0.000, batch=51\n",
            "17: loss=1.301, reward_mean=0.030, reward_bound=0.000, batch=54\n",
            "18: loss=1.296, reward_mean=0.040, reward_bound=0.000, batch=58\n",
            "19: loss=1.292, reward_mean=0.040, reward_bound=0.000, batch=62\n",
            "20: loss=1.287, reward_mean=0.030, reward_bound=0.000, batch=65\n",
            "21: loss=1.283, reward_mean=0.000, reward_bound=0.000, batch=65\n",
            "22: loss=1.282, reward_mean=0.050, reward_bound=0.000, batch=70\n",
            "23: loss=1.278, reward_mean=0.040, reward_bound=0.000, batch=74\n",
            "24: loss=1.274, reward_mean=0.050, reward_bound=0.000, batch=79\n",
            "25: loss=1.270, reward_mean=0.080, reward_bound=0.000, batch=87\n",
            "26: loss=1.265, reward_mean=0.050, reward_bound=0.000, batch=92\n",
            "27: loss=1.260, reward_mean=0.050, reward_bound=0.000, batch=97\n",
            "28: loss=1.253, reward_mean=0.080, reward_bound=0.000, batch=105\n",
            "29: loss=1.249, reward_mean=0.030, reward_bound=0.000, batch=108\n",
            "30: loss=1.245, reward_mean=0.050, reward_bound=0.000, batch=113\n",
            "31: loss=1.240, reward_mean=0.070, reward_bound=0.000, batch=120\n",
            "32: loss=1.240, reward_mean=0.100, reward_bound=0.000, batch=130\n",
            "33: loss=1.237, reward_mean=0.090, reward_bound=0.000, batch=139\n",
            "34: loss=1.235, reward_mean=0.140, reward_bound=0.000, batch=153\n",
            "35: loss=1.231, reward_mean=0.040, reward_bound=0.000, batch=157\n",
            "36: loss=1.227, reward_mean=0.080, reward_bound=0.000, batch=165\n",
            "37: loss=1.222, reward_mean=0.050, reward_bound=0.000, batch=170\n",
            "38: loss=1.217, reward_mean=0.130, reward_bound=0.000, batch=183\n",
            "39: loss=1.212, reward_mean=0.080, reward_bound=0.000, batch=191\n",
            "40: loss=1.205, reward_mean=0.120, reward_bound=0.000, batch=203\n",
            "41: loss=1.203, reward_mean=0.070, reward_bound=0.000, batch=210\n",
            "42: loss=1.198, reward_mean=0.050, reward_bound=0.000, batch=215\n",
            "43: loss=1.189, reward_mean=0.070, reward_bound=0.758, batch=220\n",
            "44: loss=1.177, reward_mean=0.090, reward_bound=0.813, batch=224\n",
            "45: loss=1.166, reward_mean=0.100, reward_bound=0.826, batch=225\n",
            "46: loss=1.146, reward_mean=0.150, reward_bound=0.843, batch=224\n",
            "47: loss=1.130, reward_mean=0.130, reward_bound=0.859, batch=227\n",
            "48: loss=1.116, reward_mean=0.110, reward_bound=0.860, batch=221\n",
            "49: loss=1.105, reward_mean=0.160, reward_bound=0.869, batch=223\n",
            "50: loss=1.085, reward_mean=0.160, reward_bound=0.878, batch=215\n",
            "51: loss=1.062, reward_mean=0.190, reward_bound=0.886, batch=207\n",
            "52: loss=1.032, reward_mean=0.130, reward_bound=0.895, batch=188\n",
            "53: loss=1.037, reward_mean=0.160, reward_bound=0.843, batch=202\n",
            "54: loss=1.019, reward_mean=0.200, reward_bound=0.886, batch=209\n",
            "55: loss=1.012, reward_mean=0.180, reward_bound=0.895, batch=219\n",
            "56: loss=1.005, reward_mean=0.110, reward_bound=0.895, batch=228\n",
            "57: loss=0.963, reward_mean=0.240, reward_bound=0.904, batch=194\n",
            "58: loss=0.958, reward_mean=0.220, reward_bound=0.913, batch=206\n",
            "59: loss=0.951, reward_mean=0.150, reward_bound=0.891, batch=214\n",
            "60: loss=0.906, reward_mean=0.160, reward_bound=0.914, batch=174\n",
            "61: loss=0.902, reward_mean=0.220, reward_bound=0.895, batch=188\n",
            "62: loss=0.898, reward_mean=0.210, reward_bound=0.896, batch=201\n",
            "63: loss=0.887, reward_mean=0.140, reward_bound=0.904, batch=207\n",
            "64: loss=0.866, reward_mean=0.270, reward_bound=0.914, batch=211\n",
            "65: loss=0.786, reward_mean=0.230, reward_bound=0.923, batch=135\n",
            "66: loss=0.808, reward_mean=0.240, reward_bound=0.000, batch=159\n",
            "67: loss=0.809, reward_mean=0.270, reward_bound=0.886, batch=182\n",
            "68: loss=0.791, reward_mean=0.240, reward_bound=0.898, batch=197\n",
            "69: loss=0.776, reward_mean=0.250, reward_bound=0.904, batch=205\n",
            "70: loss=0.749, reward_mean=0.310, reward_bound=0.914, batch=206\n",
            "71: loss=0.718, reward_mean=0.380, reward_bound=0.923, batch=202\n",
            "72: loss=0.618, reward_mean=0.310, reward_bound=0.932, batch=105\n",
            "73: loss=0.700, reward_mean=0.380, reward_bound=0.169, batch=143\n",
            "74: loss=0.657, reward_mean=0.390, reward_bound=0.895, batch=166\n",
            "75: loss=0.630, reward_mean=0.380, reward_bound=0.914, batch=181\n",
            "76: loss=0.600, reward_mean=0.300, reward_bound=0.923, batch=181\n",
            "77: loss=0.550, reward_mean=0.390, reward_bound=0.932, batch=152\n",
            "78: loss=0.547, reward_mean=0.360, reward_bound=0.923, batch=171\n",
            "79: loss=0.523, reward_mean=0.410, reward_bound=0.932, batch=171\n",
            "80: loss=0.511, reward_mean=0.450, reward_bound=0.932, batch=181\n",
            "81: loss=0.498, reward_mean=0.410, reward_bound=0.932, batch=192\n",
            "82: loss=0.488, reward_mean=0.380, reward_bound=0.932, batch=203\n",
            "84: loss=0.705, reward_mean=0.430, reward_bound=0.000, batch=43\n",
            "85: loss=0.720, reward_mean=0.490, reward_bound=0.000, batch=92\n",
            "86: loss=0.673, reward_mean=0.490, reward_bound=0.898, batch=134\n",
            "87: loss=0.624, reward_mean=0.490, reward_bound=0.904, batch=163\n",
            "88: loss=0.514, reward_mean=0.520, reward_bound=0.923, batch=158\n",
            "89: loss=0.414, reward_mean=0.650, reward_bound=0.932, batch=123\n",
            "90: loss=0.404, reward_mean=0.600, reward_bound=0.932, batch=148\n",
            "91: loss=0.397, reward_mean=0.520, reward_bound=0.933, batch=173\n",
            "93: loss=0.601, reward_mean=0.600, reward_bound=0.000, batch=60\n",
            "94: loss=0.566, reward_mean=0.600, reward_bound=0.904, batch=111\n",
            "95: loss=0.440, reward_mean=0.530, reward_bound=0.923, batch=118\n",
            "96: loss=0.362, reward_mean=0.560, reward_bound=0.932, batch=112\n",
            "97: loss=0.357, reward_mean=0.630, reward_bound=0.932, batch=143\n",
            "99: loss=0.562, reward_mean=0.650, reward_bound=0.000, batch=65\n",
            "100: loss=0.507, reward_mean=0.570, reward_bound=0.914, batch=107\n",
            "101: loss=0.442, reward_mean=0.590, reward_bound=0.923, batch=130\n",
            "102: loss=0.338, reward_mean=0.680, reward_bound=0.932, batch=133\n",
            "104: loss=0.543, reward_mean=0.690, reward_bound=0.000, batch=69\n",
            "105: loss=0.370, reward_mean=0.690, reward_bound=0.923, batch=111\n",
            "106: loss=0.313, reward_mean=0.690, reward_bound=0.932, batch=129\n",
            "107: loss=0.306, reward_mean=0.630, reward_bound=0.941, batch=171\n",
            "109: loss=0.445, reward_mean=0.650, reward_bound=0.000, batch=65\n",
            "110: loss=0.370, reward_mean=0.700, reward_bound=0.923, batch=113\n",
            "111: loss=0.294, reward_mean=0.690, reward_bound=0.932, batch=131\n",
            "113: loss=0.462, reward_mean=0.640, reward_bound=0.000, batch=64\n",
            "114: loss=0.280, reward_mean=0.730, reward_bound=0.932, batch=88\n",
            "115: loss=0.272, reward_mean=0.740, reward_bound=0.941, batch=136\n",
            "117: loss=0.369, reward_mean=0.750, reward_bound=0.923, batch=63\n",
            "118: loss=0.275, reward_mean=0.750, reward_bound=0.932, batch=91\n",
            "120: loss=0.323, reward_mean=0.760, reward_bound=0.923, batch=59\n",
            "121: loss=0.255, reward_mean=0.770, reward_bound=0.932, batch=95\n",
            "123: loss=0.403, reward_mean=0.710, reward_bound=0.908, batch=70\n",
            "124: loss=0.320, reward_mean=0.740, reward_bound=0.932, batch=120\n",
            "125: loss=0.248, reward_mean=0.730, reward_bound=0.932, batch=149\n",
            "126: loss=0.248, reward_mean=0.780, reward_bound=0.941, batch=204\n",
            "128: loss=0.233, reward_mean=0.820, reward_bound=0.932, batch=61\n",
            "130: loss=0.299, reward_mean=0.730, reward_bound=0.923, batch=63\n",
            "131: loss=0.237, reward_mean=0.840, reward_bound=0.932, batch=113\n",
            "133: loss=0.331, reward_mean=0.790, reward_bound=0.929, batch=70\n",
            "134: loss=0.299, reward_mean=0.820, reward_bound=0.932, batch=142\n",
            "136: loss=0.215, reward_mean=0.810, reward_bound=0.932, batch=51\n",
            "138: loss=0.226, reward_mean=0.880, reward_bound=0.932, batch=67\n",
            "140: loss=0.206, reward_mean=0.830, reward_bound=0.932, batch=68\n",
            "141: loss=0.199, reward_mean=0.870, reward_bound=0.941, batch=140\n",
            "143: loss=0.197, reward_mean=0.830, reward_bound=0.932, batch=65\n",
            "145: loss=0.215, reward_mean=0.790, reward_bound=0.932, batch=60\n",
            "146: loss=0.210, reward_mean=0.850, reward_bound=0.941, batch=128\n",
            "148: loss=0.209, reward_mean=0.860, reward_bound=0.932, batch=68\n",
            "149: loss=0.211, reward_mean=0.890, reward_bound=0.941, batch=140\n",
            "151: loss=0.205, reward_mean=0.890, reward_bound=0.941, batch=75\n",
            "153: loss=0.210, reward_mean=0.890, reward_bound=0.941, batch=72\n",
            "155: loss=0.198, reward_mean=0.920, reward_bound=0.941, batch=71\n",
            "157: loss=0.196, reward_mean=0.850, reward_bound=0.932, batch=68\n",
            "158: loss=0.186, reward_mean=0.900, reward_bound=0.941, batch=146\n",
            "160: loss=0.183, reward_mean=0.880, reward_bound=0.941, batch=74\n",
            "161: loss=0.190, reward_mean=0.870, reward_bound=0.941, batch=150\n",
            "163: loss=0.184, reward_mean=0.840, reward_bound=0.941, batch=75\n",
            "165: loss=0.170, reward_mean=0.900, reward_bound=0.939, batch=70\n",
            "166: loss=0.173, reward_mean=0.920, reward_bound=0.941, batch=142\n",
            "168: loss=0.191, reward_mean=0.920, reward_bound=0.941, batch=84\n",
            "169: loss=0.183, reward_mean=0.940, reward_bound=0.941, batch=162\n",
            "171: loss=0.183, reward_mean=0.900, reward_bound=0.941, batch=77\n",
            "173: loss=0.168, reward_mean=0.930, reward_bound=0.941, batch=84\n",
            "174: loss=0.166, reward_mean=0.900, reward_bound=0.941, batch=163\n",
            "176: loss=0.176, reward_mean=0.910, reward_bound=0.941, batch=81\n",
            "178: loss=0.162, reward_mean=0.920, reward_bound=0.941, batch=73\n",
            "180: loss=0.165, reward_mean=0.920, reward_bound=0.941, batch=74\n",
            "181: loss=0.168, reward_mean=0.930, reward_bound=0.941, batch=154\n",
            "183: loss=0.173, reward_mean=0.900, reward_bound=0.941, batch=80\n",
            "184: loss=0.173, reward_mean=0.930, reward_bound=0.941, batch=159\n",
            "185: loss=0.171, reward_mean=0.930, reward_bound=0.941, batch=243\n",
            "187: loss=0.171, reward_mean=0.900, reward_bound=0.941, batch=82\n",
            "189: loss=0.170, reward_mean=0.940, reward_bound=0.941, batch=86\n",
            "191: loss=0.157, reward_mean=0.900, reward_bound=0.941, batch=78\n",
            "192: loss=0.161, reward_mean=0.940, reward_bound=0.941, batch=156\n",
            "194: loss=0.153, reward_mean=0.940, reward_bound=0.941, batch=86\n",
            "196: loss=0.169, reward_mean=0.930, reward_bound=0.941, batch=85\n",
            "198: loss=0.162, reward_mean=0.890, reward_bound=0.941, batch=80\n",
            "199: loss=0.155, reward_mean=0.910, reward_bound=0.941, batch=158\n",
            "201: loss=0.152, reward_mean=0.900, reward_bound=0.941, batch=83\n",
            "203: loss=0.154, reward_mean=0.900, reward_bound=0.941, batch=82\n",
            "205: loss=0.152, reward_mean=0.920, reward_bound=0.941, batch=79\n",
            "207: loss=0.150, reward_mean=0.970, reward_bound=0.941, batch=95\n",
            "209: loss=0.163, reward_mean=0.920, reward_bound=0.941, batch=80\n",
            "210: loss=0.160, reward_mean=0.940, reward_bound=0.941, batch=171\n",
            "212: loss=0.145, reward_mean=0.940, reward_bound=0.941, batch=87\n",
            "214: loss=0.151, reward_mean=0.970, reward_bound=0.941, batch=87\n",
            "216: loss=0.154, reward_mean=0.920, reward_bound=0.941, batch=87\n",
            "218: loss=0.147, reward_mean=0.970, reward_bound=0.941, batch=90\n",
            "219: loss=0.150, reward_mean=0.960, reward_bound=0.941, batch=175\n",
            "221: loss=0.158, reward_mean=0.950, reward_bound=0.941, batch=95\n",
            "223: loss=0.172, reward_mean=0.940, reward_bound=0.941, batch=86\n",
            "225: loss=0.139, reward_mean=0.950, reward_bound=0.941, batch=87\n",
            "227: loss=0.144, reward_mean=0.910, reward_bound=0.941, batch=86\n",
            "229: loss=0.166, reward_mean=0.950, reward_bound=0.941, batch=89\n",
            "231: loss=0.155, reward_mean=1.000, reward_bound=0.941, batch=94\n",
            "Solved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pBaByvBLd_aW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#env = gym.wrappers.Monitor(env, directory=\"mon\", force=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sDN06jufTXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_n_times(env, net, n):\n",
        "  history = list()\n",
        "  \n",
        "  for i in range(n):\n",
        "    rewards = 0\n",
        "    obs = env.reset()\n",
        "    done = False\n",
        "    \n",
        "    while not done:\n",
        "      #NN gets only tensor-like information\n",
        "      obs = torch.FloatTensor([obs])\n",
        "      #getting predictions from our NN\n",
        "      probs = net(obs)\n",
        "      #using activation layer on results\n",
        "      sm = nn.Softmax(dim=1)\n",
        "      act_probs_v = sm(probs)\n",
        "      #converting to numpy data-type\n",
        "      act_probs_v.data.numpy()[0]\n",
        "      act_probs = act_probs_v.data.numpy()[0]\n",
        "      #getting index of highest prediction\n",
        "      act_probs.max()\n",
        "      action = np.where(act_probs == act_probs.max())[0][0]\n",
        "      next_obs, reward, is_done, _ = env.step(action)\n",
        "      rewards += reward\n",
        "      done = is_done\n",
        "      obs = next_obs\n",
        "      \n",
        "    history.append(rewards)\n",
        "    \n",
        "  return history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4gJPV7Ffh2R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = play_n_times(env, net, 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOUrbzo_foXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6d50c916-25cf-4332-b022-92df91b2ce8a"
      },
      "source": [
        "history[:10]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUUhorduf0Gq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = np.array(history)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRgy87pZhZKI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "59f9c5b1-2bed-4cfa-dbc9-09f2ee2f8247"
      },
      "source": [
        "print(\"Max = %d, Min = %d, Mean = %d\"%(history.max(), history.min(), history.mean()))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Max = 1, Min = 1, Mean = 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiMa2kMpha7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv-qzpqAhc9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rJpQR9rinlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}