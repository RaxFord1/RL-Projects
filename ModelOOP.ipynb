{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''\n",
    "    Agent with environment\n",
    "    '''\n",
    "    def __init__(self, env = None, learning_rate = 1e-2, seed = 0, sizes = [32], activation = tf.tanh):\n",
    "        '''Initializing all specific variables for bot'''\n",
    "        \n",
    "        assert env, \"Value env is required\"\n",
    "        \n",
    "        self.env = gym.make(env)\n",
    "        self.seed = 0\n",
    "        self.obs = self.env.reset()\n",
    "        self.activation = activation\n",
    "        self.sizes = sizes\n",
    "        self.obs_dim = self.env.observation_space.shape[0]\n",
    "        self.n_acts = self.env.action_space.n\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.obs_ph, self.logits, self.action = self.create_model()\n",
    "        self.weights_ph, self.act_ph, self.loss = self.loss_func()\n",
    "        #optimizer\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate = self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.writer = tf.summary.FileWriter('./Agent', self.sess.graph)\n",
    "        \n",
    "        \n",
    "    def create_model(self):\n",
    "        '''\n",
    "        Creating model\n",
    "        obs_dim = env.observation_space.shape -> (4,), use .shape[0]\n",
    "        sizes [hidden layers] + [env.action_space.n]\n",
    "        ''' \n",
    "        assert self.obs_dim #maybe obs_dim == 0, check this!\n",
    "        obs_ph = tf.placeholder(dtype = tf.float32, shape=(None, self.obs_dim))\n",
    "        hidden = tf.layers.dense(obs_ph, units = self.sizes[0], activation = self.activation)\n",
    "        for x in self.sizes[1:-1]:\n",
    "            hidden = tf.layers.dense(hidden, units = x, \\\n",
    "                                     activation = self.activation)\n",
    "        logits = tf.layers.dense(hidden, units = self.n_acts)\n",
    "        action = tf.squeeze(tf.multinomial(logits=logits, \\\n",
    "                                           num_samples=1), axis=1)\n",
    "\n",
    "        #action = tf.argmax(input = logits, axis = 1)\n",
    "\n",
    "        return obs_ph, logits, action\n",
    "        \n",
    "    \n",
    "    def act(self, obs):\n",
    "        '''Choosing action for self.obs '''\n",
    "        action = self.sess.run(self.action, feed_dict = {\n",
    "            self.obs_ph : [obs]\n",
    "        })#[0] or [1]\n",
    "        return action[0]\n",
    "    \n",
    "    \n",
    "    def play_episode(self, rendering = False):\n",
    "        '''\n",
    "        Playing only one episode, collecting trajectory and rewards\n",
    "        '''\n",
    "        episode_obs_batch = list()\n",
    "        episode_action_batch = list()\n",
    "        episode_trajectory = list()\n",
    "        rewards = list()\n",
    "        \n",
    "        self.obs = self.env.reset()\n",
    "        while True:\n",
    "            episode_obs_batch.append(self.obs.copy())\n",
    "            if rendering == True:\n",
    "                self.env.render()\n",
    "            action = self.act(self.obs)\n",
    "            episode_trajectory.append((self.obs, action))\n",
    "            self.obs, reward, is_done, info = self.env.step(action)\n",
    "            \n",
    "            \n",
    "            episode_action_batch.append(action)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            if is_done:\n",
    "                break\n",
    "        if rendering == True:\n",
    "            return sum(rewards)\n",
    "        else:\n",
    "            return rewards, episode_obs_batch, episode_action_batch, episode_trajectory\n",
    "    \n",
    "    \n",
    "    def play_epoch(self, n):\n",
    "        '''\n",
    "        Collecting samples with current policy\n",
    "        n = number of samples from 1 epoch\n",
    "        '''\n",
    "        epoch_obs_batch = list()\n",
    "        epoch_action_batch = list()\n",
    "        epoch_rewards = list()\n",
    "        epoch_trajectory = list()\n",
    "        epoch_reward_to_go = list()\n",
    "        \n",
    "        while len(epoch_obs_batch)<n:\n",
    "            reward, obs_batch, action_batch, trajectory_batch = self.play_episode()\n",
    "            \n",
    "            epoch_obs_batch+=(obs_batch)\n",
    "            \n",
    "            epoch_action_batch+=(action_batch)\n",
    "            epoch_rewards.append(sum(reward))\n",
    "            epoch_reward_to_go += list(self.reward_to_go(reward))\n",
    "            \n",
    "            epoch_trajectory.append(trajectory_batch)\n",
    "        return epoch_obs_batch, epoch_action_batch, epoch_trajectory, epoch_reward_to_go, sum(epoch_rewards)/len(epoch_rewards)\n",
    "    \n",
    "    \n",
    "    def reward_to_go(self, rews):\n",
    "        '''\n",
    "        Calculating parameterized policy(weights)\n",
    "        '''\n",
    "        n = len(rews)\n",
    "        rtgs = np.zeros_like(rews)\n",
    "        for i in reversed(range(n)):\n",
    "            rtgs[i] = rews[i] + (rtgs[i+1] if i+1 < n else 0)\n",
    "        #returns [200. 199. ... 1.]\n",
    "        return rtgs\n",
    "    \n",
    "    \n",
    "    def train_epoch(self):\n",
    "        '''\n",
    "        Training agent for n epochs\n",
    "        '''\n",
    "        obs_batches, action_batches, trajectory_batches, reward_to_go, mean_reward = self.play_epoch(5000)\n",
    "        self.play_episode(True)\n",
    "        #print(\"obs: {}, \\nactions: {},\\n rewards: {}\".format(obs_batches[0:8], action_batches[0:8], reward_to_go[0:8]))\n",
    "        loss, _ = self.optimize(obs_batches, action_batches, reward_to_go)\n",
    "        #print(np.array(obs_batches).shape,np.array(action_batches).shape,np.array(reward_to_go).shape)\n",
    "        return loss, mean_reward\n",
    "        \n",
    "    def can_solve(self):\n",
    "        for i in range(100):\n",
    "            reward, _, _, _= self.play_episode()\n",
    "            if sum(reward) != 200: \n",
    "                print(\"{} Cannot solve yet {}\".format(i, sum(reward)))\n",
    "                return False\n",
    "        return True\n",
    "                \n",
    "        \n",
    "    def train_n_epochs(self, n):\n",
    "        for i in range(n):\n",
    "            loss, reward = self.train_epoch()\n",
    "            if reward == 200:\n",
    "                if self.can_solve():\n",
    "                    return \"SOLVED\"\n",
    "                    \n",
    "            print(\"#i:{} loss: {}, mean reward: {}\".format(i, loss, reward))    \n",
    "            \n",
    "            \n",
    "    def loss_func(self):\n",
    "        '''Calculating loss, returning specific placeholders'''\n",
    "        weights_ph = tf.placeholder(shape=(None,), dtype = tf.float32)\n",
    "        act_ph = tf.placeholder(shape=(None,), dtype = tf.int32)\n",
    "        \n",
    "        action_masks = tf.one_hot(act_ph, self.n_acts)#array([[0., 1.]...], dtype=float32)\n",
    "\n",
    "        log_probs = tf.reduce_sum(action_masks * tf.nn.log_softmax(self.logits), axis=1)\n",
    "        #array([-0.6952652,...], dtype=float32)\n",
    "        loss = -tf.reduce_mean(weights_ph* log_probs)#9.812662\n",
    "        return weights_ph, act_ph, loss\n",
    "    \n",
    "    \n",
    "    def optimize(self, obs, act, weights):\n",
    "        '''Optimizing logits with loss, using self.train_op(optimizer)'''\n",
    "        return self.sess.run([self.loss, self.train_op], feed_dict = {\n",
    "            self.obs_ph : obs,\n",
    "            self.act_ph: act,\n",
    "            self.weights_ph: weights\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-8b3298d2db7a>:38: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff32710f7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff32710f7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff32710f7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff32710f7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff32710f7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff32710f7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff32710f7f0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x7ff32710f7f0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:From <ipython-input-2-8b3298d2db7a>:42: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.random.categorical` instead.\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#i:0 loss: 16.247610092163086, mean reward: 34.36986301369863\n",
      "#i:1 loss: 20.362260818481445, mean reward: 42.63025210084034\n",
      "#i:2 loss: 17.805347442626953, mean reward: 43.69827586206897\n",
      "#i:3 loss: 22.276695251464844, mean reward: 50.666666666666664\n",
      "#i:4 loss: 21.84754753112793, mean reward: 52.8421052631579\n",
      "#i:5 loss: 24.016469955444336, mean reward: 59.98809523809524\n",
      "#i:6 loss: 24.211071014404297, mean reward: 65.24675324675324\n",
      "#i:7 loss: 26.022903442382812, mean reward: 68.4931506849315\n",
      "#i:8 loss: 28.256132125854492, mean reward: 74.97058823529412\n",
      "#i:9 loss: 30.995515823364258, mean reward: 84.79661016949153\n",
      "#i:10 loss: 38.82268142700195, mean reward: 103.63265306122449\n",
      "#i:11 loss: 38.121517181396484, mean reward: 106.46808510638297\n",
      "#i:12 loss: 43.11361312866211, mean reward: 121.66666666666667\n",
      "#i:13 loss: 43.52337646484375, mean reward: 130.53846153846155\n",
      "#i:14 loss: 46.64395523071289, mean reward: 145.11428571428573\n",
      "#i:15 loss: 49.141937255859375, mean reward: 154.0909090909091\n",
      "#i:16 loss: 51.701297760009766, mean reward: 170.16666666666666\n",
      "#i:17 loss: 52.62766647338867, mean reward: 177.68965517241378\n",
      "#i:18 loss: 53.160221099853516, mean reward: 180.96428571428572\n",
      "#i:19 loss: 51.90282440185547, mean reward: 178.96428571428572\n",
      "#i:20 loss: 48.893890380859375, mean reward: 165.1290322580645\n",
      "#i:21 loss: 51.56550598144531, mean reward: 181.42857142857142\n",
      "#i:22 loss: 49.93798828125, mean reward: 175.0\n",
      "#i:23 loss: 49.202030181884766, mean reward: 174.93103448275863\n",
      "#i:24 loss: 50.82562255859375, mean reward: 178.89285714285714\n",
      "#i:25 loss: 50.89562225341797, mean reward: 186.22222222222223\n",
      "#i:26 loss: 51.66584777832031, mean reward: 190.40740740740742\n",
      "#i:27 loss: 51.53873062133789, mean reward: 187.62962962962962\n",
      "#i:28 loss: 53.077247619628906, mean reward: 199.84615384615384\n",
      "#i:29 loss: 52.710968017578125, mean reward: 199.8846153846154\n",
      "13 Cannot solve yet 161.0\n",
      "#i:30 loss: 52.80084228515625, mean reward: 200.0\n",
      "#i:31 loss: 52.455562591552734, mean reward: 198.84615384615384\n",
      "38 Cannot solve yet 194.0\n",
      "#i:32 loss: 52.39229202270508, mean reward: 200.0\n",
      "#i:33 loss: 50.889503479003906, mean reward: 198.57692307692307\n",
      "#i:34 loss: 51.059627532958984, mean reward: 199.8846153846154\n",
      "#i:35 loss: 51.66870880126953, mean reward: 199.80769230769232\n",
      "23 Cannot solve yet 181.0\n",
      "#i:36 loss: 50.500457763671875, mean reward: 200.0\n",
      "31 Cannot solve yet 156.0\n",
      "#i:37 loss: 51.244876861572266, mean reward: 200.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SOLVED'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.train_n_epochs(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.play_episode(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveRewardLimitation(gym.ActionWrapper):\n",
    "    '''this class serves to escape 200 rewards = done'''\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "        state = self.state\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "        force = self.force_mag if action==1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta* temp) / (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))\n",
    "        xacc  = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x  = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else: # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x  = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "        self.state = (x,x_dot,theta,theta_dot)\n",
    "        done =  x < -self.x_threshold \\\n",
    "                or x > self.x_threshold \\\n",
    "                or theta < -self.theta_threshold_radians \\\n",
    "                or theta > self.theta_threshold_radians\n",
    "        done = bool(done)\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                print(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "        return np.array(self.state), reward, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        '''@https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py'''\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold*2\n",
    "        scale = screen_width/world_width\n",
    "        carty = 100 # TOP OF CART\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l,r,t,b = -cartwidth/2, cartwidth/2, cartheight/2, -cartheight/2\n",
    "            axleoffset =cartheight/4.0\n",
    "            cart = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "            l,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n",
    "            pole = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "            pole.set_color(.8,.6,.4)\n",
    "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "            pole.add_attr(self.poletrans)\n",
    "            pole.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(pole)\n",
    "            self.axle = rendering.make_circle(polewidth/2)\n",
    "            self.axle.add_attr(self.poletrans)\n",
    "            self.axle.add_attr(self.carttrans)\n",
    "            self.axle.set_color(.5,.5,.8)\n",
    "            self.viewer.add_geom(self.axle)\n",
    "            self.track = rendering.Line((0,carty), (screen_width,carty))\n",
    "            self.track.set_color(0,0,0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            self._pole_geom = pole\n",
    "\n",
    "        if self.state is None: return None\n",
    "\n",
    "        # Edit the pole polygon vertex\n",
    "        pole = self._pole_geom\n",
    "        l,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n",
    "        pole.v = [(l,b), (l,t), (r,t), (r,b)]\n",
    "\n",
    "        x = self.state\n",
    "        cartx = x[0]*scale+screen_width/2.0 # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "        self.poletrans.set_rotation(-x[2])\n",
    "\n",
    "        return self.viewer.render(return_rgb_array = mode=='rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env = RemoveRewardLimitation(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.4061323  -0.89856719 -0.07917451 -0.10530031] 329.0\n"
     ]
    }
   ],
   "source": [
    "rewards = 0\n",
    "obs = env.reset()\n",
    "is_done = False\n",
    "while is_done == False:\n",
    "    env.render()\n",
    "    action = agent.sess.run(agent.action, {agent.obs_ph: [obs]})[0]\n",
    "    obs, reward, is_done, _ = env.step(action)\n",
    "    rewards+=reward\n",
    "    if is_done:\n",
    "        print(obs, rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
