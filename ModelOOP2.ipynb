{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    '''\n",
    "    Agent with environment\n",
    "    '''\n",
    "    def __init__(self, env = None, learning_rate = 1e-2, seed = 0, sizes = [32], activation = tf.tanh):\n",
    "        '''Initializing all specific variables for bot'''\n",
    "        \n",
    "        assert env, \"Value env is required\"\n",
    "        \n",
    "        self.env = gym.make(env)\n",
    "        self.seed = 0\n",
    "        self.obs = self.env.reset()\n",
    "        self.activation = activation\n",
    "        self.sizes = sizes\n",
    "        self.obs_dim = self.env.observation_space.shape[0]\n",
    "        self.n_acts = self.env.action_space.n\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.obs_ph, self.logits, self.action = self.create_model()\n",
    "        self.weights_ph, self.act_ph, self.loss = self.loss_func()\n",
    "        #optimizer\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate = self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        self.sess = tf.InteractiveSession()\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        self.writer = tf.summary.FileWriter('./Agent', self.sess.graph)\n",
    "        \n",
    "        \n",
    "    def create_model(self):\n",
    "        '''\n",
    "        Creating model\n",
    "        obs_dim = env.observation_space.shape -> (4,), use .shape[0]\n",
    "        sizes [hidden layers] + [env.action_space.n]\n",
    "        ''' \n",
    "        assert self.obs_dim #maybe obs_dim == 0, check this!\n",
    "        obs_ph = tf.placeholder(dtype = tf.float32, shape=(None, self.obs_dim))\n",
    "        hidden = tf.layers.dense(obs_ph, units = self.sizes[0], activation = self.activation)\n",
    "        for x in self.sizes[1:-1]:\n",
    "            hidden = tf.layers.dense(hidden, units = x, \\\n",
    "                                     activation = self.activation)\n",
    "        logits = tf.layers.dense(hidden, units = self.n_acts)\n",
    "        action = tf.squeeze(tf.multinomial(logits=logits, \\\n",
    "                                           num_samples=1), axis=1)\n",
    "\n",
    "        #action = tf.argmax(input = logits, axis = 1)\n",
    "\n",
    "        return obs_ph, logits, action\n",
    "        \n",
    "    \n",
    "    def act(self, obs):\n",
    "        '''Choosing action for self.obs '''\n",
    "        action = self.sess.run(self.action, feed_dict = {\n",
    "            self.obs_ph : [obs]\n",
    "        })#[0] or [1]\n",
    "        return action[0]\n",
    "    \n",
    "    \n",
    "    def play_episode(self, rendering = False):\n",
    "        '''\n",
    "        Playing only one episode, collecting trajectory and rewards\n",
    "        '''\n",
    "        episode_obs_batch = list()\n",
    "        episode_action_batch = list()\n",
    "        episode_trajectory = list()\n",
    "        rewards = list()\n",
    "        \n",
    "        self.obs = self.env.reset()\n",
    "        while True:\n",
    "            episode_obs_batch.append(self.obs.copy())\n",
    "            if rendering == True:\n",
    "                self.env.render()\n",
    "            action = self.act(self.obs)\n",
    "            episode_trajectory.append((self.obs, action))\n",
    "            self.obs, reward, is_done, info = self.env.step(action)\n",
    "            \n",
    "            \n",
    "            episode_action_batch.append(action)\n",
    "            \n",
    "            rewards.append(reward)\n",
    "            if is_done:\n",
    "                break\n",
    "        if rendering == True:\n",
    "            return sum(rewards)\n",
    "        else:\n",
    "            return rewards, episode_obs_batch, episode_action_batch, episode_trajectory\n",
    "    \n",
    "    \n",
    "    def play_epoch(self, n):\n",
    "        '''\n",
    "        Collecting samples with current policy\n",
    "        n = number of samples from 1 epoch\n",
    "        '''\n",
    "        epoch_obs_batch = list()\n",
    "        epoch_action_batch = list()\n",
    "        epoch_rewards = list()\n",
    "        epoch_trajectory = list()\n",
    "        epoch_reward_to_go = list()\n",
    "        \n",
    "        while len(epoch_obs_batch)<n:\n",
    "            reward, obs_batch, action_batch, trajectory_batch = self.play_episode()\n",
    "            \n",
    "            epoch_obs_batch+=(obs_batch)\n",
    "            \n",
    "            epoch_action_batch+=(action_batch)\n",
    "            epoch_rewards.append(sum(reward))\n",
    "            epoch_reward_to_go += list(self.reward_to_go(reward))\n",
    "            \n",
    "            epoch_trajectory.append(trajectory_batch)\n",
    "        return epoch_obs_batch, epoch_action_batch, epoch_trajectory, epoch_reward_to_go, sum(epoch_rewards)/len(epoch_rewards)\n",
    "    \n",
    "    \n",
    "    def reward_to_go(self, rews):\n",
    "        '''\n",
    "        Calculating parameterized policy(weights)\n",
    "        '''\n",
    "        n = len(rews)\n",
    "        rtgs = np.zeros_like(rews)\n",
    "        for i in reversed(range(n)):\n",
    "            rtgs[i] = rews[i] + (rtgs[i+1] if i+1 < n else 0)\n",
    "        #returns [200. 199. ... 1.]\n",
    "        return rtgs\n",
    "    \n",
    "    \n",
    "    def train_epoch(self):\n",
    "        '''\n",
    "        Training agent for n epochs\n",
    "        '''\n",
    "        obs_batches, action_batches, trajectory_batches, reward_to_go, mean_reward = self.play_epoch(5000)\n",
    "        self.play_episode(True)\n",
    "        #print(\"obs: {}, \\nactions: {},\\n rewards: {}\".format(obs_batches[0:8], action_batches[0:8], reward_to_go[0:8]))\n",
    "        loss, _ = self.optimize(obs_batches, action_batches, reward_to_go)\n",
    "        #print(np.array(obs_batches).shape,np.array(action_batches).shape,np.array(reward_to_go).shape)\n",
    "        return loss, mean_reward\n",
    "        \n",
    "    def can_solve(self):\n",
    "        for i in range(100):\n",
    "            reward, _, _, _= self.play_episode()\n",
    "            if sum(reward) != 200: \n",
    "                print(\"{} Cannot solve yet {}\".format(i, sum(reward)))\n",
    "                return False\n",
    "        return True\n",
    "                \n",
    "        \n",
    "    def train_n_epochs(self, n):\n",
    "        for i in range(n):\n",
    "            loss, reward = self.train_epoch()\n",
    "            if reward == 200:\n",
    "                if self.can_solve():\n",
    "                    return \"SOLVED\"\n",
    "                    \n",
    "            print(\"#i:{} loss: {}, mean reward: {}\".format(i, loss, reward))    \n",
    "            \n",
    "            \n",
    "    def loss_func(self):\n",
    "        '''Calculating loss, returning specific placeholders'''\n",
    "        weights_ph = tf.placeholder(shape=(None,), dtype = tf.float32)\n",
    "        act_ph = tf.placeholder(shape=(None,), dtype = tf.int32)\n",
    "        \n",
    "        action_masks = tf.one_hot(act_ph, self.n_acts)#array([[0., 1.]...], dtype=float32)\n",
    "\n",
    "        log_probs = tf.reduce_sum(action_masks * tf.nn.log_softmax(self.logits), axis=1)\n",
    "        #array([-0.6952652,...], dtype=float32)\n",
    "        loss = -tf.reduce_mean(weights_ph* log_probs)#9.812662\n",
    "        return weights_ph, act_ph, loss\n",
    "    \n",
    "    \n",
    "    def optimize(self, obs, act, weights):\n",
    "        '''Optimizing logits with loss, using self.train_op(optimizer)'''\n",
    "        return self.sess.run([self.loss, self.train_op], feed_dict = {\n",
    "            self.obs_ph : obs,\n",
    "            self.act_ph: act,\n",
    "            self.weights_ph: weights\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnregisteredEnv",
     "evalue": "No registered env with id: InvertedPendulumPyBulletEnv-v0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'InvertedPendulumPyBulletEnv-v0'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnregisteredEnv\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-2ef5e2d65d5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"InvertedPendulumPyBulletEnv-v0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-fa5d5cfda275>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env, learning_rate, seed, sizes, activation)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Value env is required\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Making new env: %s'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m         \u001b[0mspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;31m# We used to have people override _reset/_step rather than\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mspec\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDeprecatedEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Env {} not found (valid versions include {})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmatching_envs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnregisteredEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'No registered env with id: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnregisteredEnv\u001b[0m: No registered env with id: InvertedPendulumPyBulletEnv-v0"
     ]
    }
   ],
   "source": [
    "agent = Agent(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#i:0 loss: 17.38859748840332, mean reward: 37.65413533834587\n",
      "#i:1 loss: 18.03504180908203, mean reward: 39.39370078740158\n",
      "#i:2 loss: 16.7275390625, mean reward: 42.10084033613445\n",
      "#i:3 loss: 19.99949836730957, mean reward: 49.65346534653465\n",
      "#i:4 loss: 19.825490951538086, mean reward: 53.340425531914896\n",
      "#i:5 loss: 21.259174346923828, mean reward: 59.11764705882353\n",
      "#i:6 loss: 22.01289176940918, mean reward: 59.28235294117647\n",
      "#i:7 loss: 23.762725830078125, mean reward: 63.2375\n",
      "#i:8 loss: 28.84112548828125, mean reward: 77.3076923076923\n",
      "#i:9 loss: 28.648921966552734, mean reward: 79.5\n",
      "#i:10 loss: 28.27061653137207, mean reward: 81.29032258064517\n",
      "#i:11 loss: 34.772422790527344, mean reward: 101.06\n",
      "#i:12 loss: 34.323673248291016, mean reward: 104.6875\n",
      "#i:13 loss: 41.642452239990234, mean reward: 123.14634146341463\n",
      "#i:14 loss: 45.572906494140625, mean reward: 150.14705882352942\n",
      "#i:15 loss: 48.93723678588867, mean reward: 158.71875\n",
      "#i:16 loss: 50.05317687988281, mean reward: 171.3\n",
      "#i:17 loss: 54.614688873291016, mean reward: 191.37037037037038\n",
      "#i:18 loss: 53.287147521972656, mean reward: 182.21428571428572\n",
      "#i:19 loss: 52.03348159790039, mean reward: 181.5\n",
      "#i:20 loss: 52.5686149597168, mean reward: 182.10714285714286\n",
      "#i:21 loss: 54.95078659057617, mean reward: 195.19230769230768\n",
      "43 Cannot solve yet 195.0\n",
      "#i:22 loss: 54.49296188354492, mean reward: 200.0\n",
      "#i:23 loss: 53.48530197143555, mean reward: 193.65384615384616\n",
      "#i:24 loss: 54.81239700317383, mean reward: 199.6153846153846\n",
      "#i:25 loss: 54.12876892089844, mean reward: 199.69230769230768\n",
      "#i:26 loss: 53.211883544921875, mean reward: 194.0\n",
      "#i:27 loss: 52.87952423095703, mean reward: 192.07407407407408\n",
      "#i:28 loss: 51.685848236083984, mean reward: 194.34615384615384\n",
      "#i:29 loss: 52.73015594482422, mean reward: 196.73076923076923\n",
      "#i:30 loss: 52.19694900512695, mean reward: 195.73076923076923\n",
      "#i:31 loss: 50.89635467529297, mean reward: 188.55555555555554\n",
      "#i:32 loss: 51.52284622192383, mean reward: 193.3846153846154\n",
      "#i:33 loss: 51.32479476928711, mean reward: 192.44444444444446\n",
      "#i:34 loss: 50.61945343017578, mean reward: 193.92307692307693\n",
      "#i:35 loss: 49.84794998168945, mean reward: 187.5185185185185\n",
      "#i:36 loss: 51.69513702392578, mean reward: 197.19230769230768\n",
      "#i:37 loss: 50.77727508544922, mean reward: 194.19230769230768\n",
      "#i:38 loss: 51.20174026489258, mean reward: 193.96153846153845\n",
      "#i:39 loss: 50.361263275146484, mean reward: 193.0\n",
      "#i:40 loss: 51.255157470703125, mean reward: 197.92307692307693\n",
      "#i:41 loss: 51.5819091796875, mean reward: 196.69230769230768\n",
      "#i:42 loss: 50.904598236083984, mean reward: 196.5\n",
      "#i:43 loss: 51.11173629760742, mean reward: 197.19230769230768\n",
      "4 Cannot solve yet 198.0\n",
      "#i:44 loss: 51.43145751953125, mean reward: 200.0\n",
      "#i:45 loss: 51.65194320678711, mean reward: 198.3846153846154\n",
      "39 Cannot solve yet 181.0\n",
      "#i:46 loss: 51.779029846191406, mean reward: 200.0\n",
      "#i:47 loss: 52.661808013916016, mean reward: 199.84615384615384\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'SOLVED'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.train_n_epochs(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.play_episode(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RemoveRewardLimitation(gym.ActionWrapper):\n",
    "    '''this class serves to escape 200 rewards = done'''\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        \n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action), \"%r (%s) invalid\"%(action, type(action))\n",
    "        state = self.state\n",
    "        x, x_dot, theta, theta_dot = state\n",
    "        force = self.force_mag if action==1 else -self.force_mag\n",
    "        costheta = math.cos(theta)\n",
    "        sintheta = math.sin(theta)\n",
    "        temp = (force + self.polemass_length * theta_dot * theta_dot * sintheta) / self.total_mass\n",
    "        thetaacc = (self.gravity * sintheta - costheta* temp) / (self.length * (4.0/3.0 - self.masspole * costheta * costheta / self.total_mass))\n",
    "        xacc  = temp - self.polemass_length * thetaacc * costheta / self.total_mass\n",
    "        if self.kinematics_integrator == 'euler':\n",
    "            x  = x + self.tau * x_dot\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "        else: # semi-implicit euler\n",
    "            x_dot = x_dot + self.tau * xacc\n",
    "            x  = x + self.tau * x_dot\n",
    "            theta_dot = theta_dot + self.tau * thetaacc\n",
    "            theta = theta + self.tau * theta_dot\n",
    "        self.state = (x,x_dot,theta,theta_dot)\n",
    "        done =  x < -self.x_threshold \\\n",
    "                or x > self.x_threshold \\\n",
    "                or theta < -self.theta_threshold_radians \\\n",
    "                or theta > self.theta_threshold_radians\n",
    "        done = bool(done)\n",
    "        if not done:\n",
    "            reward = 1.0\n",
    "        elif self.steps_beyond_done is None:\n",
    "            # Pole just fell!\n",
    "            self.steps_beyond_done = 0\n",
    "            reward = 1.0\n",
    "        else:\n",
    "            if self.steps_beyond_done == 0:\n",
    "                print(\"You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\")\n",
    "            self.steps_beyond_done += 1\n",
    "            reward = 0.0\n",
    "        return np.array(self.state), reward, done, {}\n",
    "    \n",
    "    def reset(self):\n",
    "        '''@https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py'''\n",
    "        self.state = self.np_random.uniform(low=-0.05, high=0.05, size=(4,))\n",
    "        self.steps_beyond_done = None\n",
    "        return np.array(self.state)\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        screen_width = 600\n",
    "        screen_height = 400\n",
    "\n",
    "        world_width = self.x_threshold*2\n",
    "        scale = screen_width/world_width\n",
    "        carty = 100 # TOP OF CART\n",
    "        polewidth = 10.0\n",
    "        polelen = scale * (2 * self.length)\n",
    "        cartwidth = 50.0\n",
    "        cartheight = 30.0\n",
    "\n",
    "        if self.viewer is None:\n",
    "            from gym.envs.classic_control import rendering\n",
    "            self.viewer = rendering.Viewer(screen_width, screen_height)\n",
    "            l,r,t,b = -cartwidth/2, cartwidth/2, cartheight/2, -cartheight/2\n",
    "            axleoffset =cartheight/4.0\n",
    "            cart = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "            self.carttrans = rendering.Transform()\n",
    "            cart.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(cart)\n",
    "            l,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n",
    "            pole = rendering.FilledPolygon([(l,b), (l,t), (r,t), (r,b)])\n",
    "            pole.set_color(.8,.6,.4)\n",
    "            self.poletrans = rendering.Transform(translation=(0, axleoffset))\n",
    "            pole.add_attr(self.poletrans)\n",
    "            pole.add_attr(self.carttrans)\n",
    "            self.viewer.add_geom(pole)\n",
    "            self.axle = rendering.make_circle(polewidth/2)\n",
    "            self.axle.add_attr(self.poletrans)\n",
    "            self.axle.add_attr(self.carttrans)\n",
    "            self.axle.set_color(.5,.5,.8)\n",
    "            self.viewer.add_geom(self.axle)\n",
    "            self.track = rendering.Line((0,carty), (screen_width,carty))\n",
    "            self.track.set_color(0,0,0)\n",
    "            self.viewer.add_geom(self.track)\n",
    "\n",
    "            self._pole_geom = pole\n",
    "\n",
    "        if self.state is None: return None\n",
    "\n",
    "        # Edit the pole polygon vertex\n",
    "        pole = self._pole_geom\n",
    "        l,r,t,b = -polewidth/2,polewidth/2,polelen-polewidth/2,-polewidth/2\n",
    "        pole.v = [(l,b), (l,t), (r,t), (r,b)]\n",
    "\n",
    "        x = self.state\n",
    "        cartx = x[0]*scale+screen_width/2.0 # MIDDLE OF CART\n",
    "        self.carttrans.set_translation(cartx, carty)\n",
    "        self.poletrans.set_rotation(-x[2])\n",
    "\n",
    "        return self.viewer.render(return_rgb_array = mode=='rgb_array')\n",
    "\n",
    "    def close(self):\n",
    "        if self.viewer:\n",
    "            self.viewer.close()\n",
    "            self.viewer = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "env = RemoveRewardLimitation(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.4061323  -0.89856719 -0.07917451 -0.10530031] 329.0\n"
     ]
    }
   ],
   "source": [
    "rewards = 0\n",
    "obs = env.reset()\n",
    "is_done = False\n",
    "while is_done == False:\n",
    "    env.render()\n",
    "    action = agent.sess.run(agent.action, {agent.obs_ph: [obs]})[0]\n",
    "    obs, reward, is_done, _ = env.step(action)\n",
    "    rewards+=reward\n",
    "    if is_done:\n",
    "        print(obs, rewards)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
